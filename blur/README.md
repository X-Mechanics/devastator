# Blur: Transformers for text (or any symbolic) data

This repository contains the code in both **PyTorch**. The boiler-plate code is based on
>[Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context](http://arxiv.org/abs/1901.02860)

>Zihang Dai\*, Zhilin Yang\*, Yiming Yang, Jaime Carbonell, Quoc V. Le, Ruslan Salakhutdinov (*: equal contribution)

>Preprint 2018

## Introduction

This directory contains our pytorch implementation of Transformer-XL.
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de10bae3",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1393b0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.join(\"..\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e755deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import time\n",
    "import math\n",
    "import os\n",
    "import itertools\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import dataclasses\n",
    "from dataclasses import dataclass\n",
    "from tqdm import tqdm\n",
    "\n",
    "from configs.xlmodelconfig import XlModelConfig\n",
    "from configs.fnetarmodelconfig import FnetarModelConfig\n",
    "from configs.feedbackmodelconfig import FeedbackModelConfig\n",
    "\n",
    "from configs.xladaptiveconfig import XlAdaptiveConfig\n",
    "from configs.feedbackadaptiveconfig import FeedbackAdaptiveConfig\n",
    "\n",
    "from configs.xldataconfig import XlDataConfig\n",
    "from configs.feedbackdataconfig import FeedbackDataConfig\n",
    "\n",
    "from configs.runconfig import RunConfig\n",
    "from configs.optimizerconfig import OptimizerConfig\n",
    "\n",
    "from blur import Blur\n",
    "\n",
    "from models.xl import Xl\n",
    "from models.fnetar import Fnetar\n",
    "from models.feedback import Feedback\n",
    "\n",
    "from modules.xlmemories import XlMemories\n",
    "from modules.feedbackmemories import FeedbackMemories\n",
    "\n",
    "from modules.adaptiveinput import AdaptiveInput\n",
    "from modules.adaptivelogsoftmax import AdaptiveLogSoftmax\n",
    "\n",
    "from utils.data_utils import get_lm_corpus\n",
    "from utils.exp_utils import create_exp_dir\n",
    "\n",
    "from models.utils.normaluniforminitializer import NormalUniformInitializer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eceb0f19",
   "metadata": {},
   "source": [
    "## Model and data arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "458a69d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Arguments:\n",
    "    model_name: str\n",
    "    dataset: str = 'wt103'\n",
    "    data: str = '../../data/wikitext-103'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2d6467",
   "metadata": {},
   "source": [
    "### Choose which model to train from ['xl', 'fnetar', 'feedback']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70aa1b28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training new feedback model\n"
     ]
    }
   ],
   "source": [
    "args = Arguments(model_name = 'feedback')\n",
    "print('Training new {} model'.format(args.model_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6609337",
   "metadata": {},
   "source": [
    "## Setup checkpoint and device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "706f4162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment dir : LM-TFM\\20211027-194915\n"
     ]
    }
   ],
   "source": [
    "run_config = RunConfig()\n",
    "optimizer_config = OptimizerConfig()\n",
    "\n",
    "run_config.work_dir = os.path.join(run_config.work_dir, time.strftime('%Y%m%d-%H%M%S'))\n",
    "logging = create_exp_dir(run_config.work_dir, scripts_to_save=['../train.py', '../blur.py'], debug=run_config.debug)\n",
    "\n",
    "# Set the random seed manually for reproducibility.\n",
    "np.random.seed(run_config.seed)\n",
    "torch.manual_seed(run_config.seed)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    if not run_config.cuda:\n",
    "        device = torch.device('cpu')\n",
    "        print('WARNING: You have a CUDA device, so you should probably run with --cuda')\n",
    "    else:\n",
    "        device = torch.device('cuda')\n",
    "        torch.cuda.manual_seed_all(run_config.seed)\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbaa866",
   "metadata": {},
   "source": [
    "## Load data and construct model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b88623c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cached dataset...\n"
     ]
    }
   ],
   "source": [
    "corpus = get_lm_corpus(args.data, args.dataset)\n",
    "\n",
    "if args.model_name == 'xl':\n",
    "    data_config = XlDataConfig()\n",
    "    adaptive_config = XlAdaptiveConfig(n_classes=len(corpus.vocab))\n",
    "    model_config = XlModelConfig()\n",
    "    transformer = Xl(**dataclasses.asdict(model_config))\n",
    "elif args.model_name == 'fnetar':\n",
    "    data_config = XlDataConfig()\n",
    "    adaptive_config = XlAdaptiveConfig(n_classes=len(corpus.vocab))\n",
    "    model_config = FnetarModelConfig()\n",
    "    transformer = Fnetar(**dataclasses.asdict(model_config))\n",
    "elif args.model_name == 'feedback':\n",
    "    data_config = FeedbackDataConfig()\n",
    "    adaptive_config = FeedbackAdaptiveConfig(n_classes=len(corpus.vocab))\n",
    "    model_config = FeedbackModelConfig()\n",
    "    transformer = Feedback(**dataclasses.asdict(model_config))\n",
    "else:\n",
    "    raise ValueError\n",
    "    \n",
    "assert data_config.batch_size % data_config.batch_chunk == 0\n",
    "\n",
    "tr_iter = corpus.get_iterator('train', data_config.batch_size, data_config.tgt_len,\n",
    "    device=device, ext_len=0)\n",
    "va_iter = corpus.get_iterator('valid', data_config.eval_batch_size, data_config.eval_tgt_len,\n",
    "    device=device, ext_len=0)\n",
    "te_iter = corpus.get_iterator('test', data_config.eval_batch_size, data_config.eval_tgt_len,\n",
    "    device=device, ext_len=0)\n",
    "\n",
    "encoder = AdaptiveInput(**dataclasses.asdict(adaptive_config))\n",
    "decoder = AdaptiveLogSoftmax(**dataclasses.asdict(adaptive_config))\n",
    "model = Blur(encoder=encoder, transformer=transformer, decoder=decoder, tie_weight=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab77c832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "    - model_name : feedback\n",
      "    - dataset : wt103\n",
      "    - data : ../../data/wikitext-103\n",
      "    - n_all_param : 159206177\n",
      "    - n_nonemb_param : 21856583\n",
      "    - n_encoder_param : 137080320\n",
      "    - n_decoder_param : 137349594\n",
      "====================================================================================================\n",
      "#params = 159206177\n",
      "#non emb params = 21856583\n",
      "#encoder params = 137080320\n",
      "#decoder params = 137349594\n"
     ]
    }
   ],
   "source": [
    "initializer = NormalUniformInitializer()\n",
    "model.apply(initializer)\n",
    "model.encoder.apply(initializer) # ensure embedding init is not overridden by out_layer in case of weight sharing\n",
    "\n",
    "args.n_all_param = sum([p.nelement() for p in model.parameters()])\n",
    "args.n_nonemb_param = sum([p.nelement() for p in model.transformer.parameters()])\n",
    "args.n_encoder_param = sum([p.nelement() for p in model.encoder.parameters()])\n",
    "args.n_decoder_param = sum([p.nelement() for p in model.decoder.parameters()])\n",
    "\n",
    "para_model = model.to(device)\n",
    "\n",
    "#### optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=optimizer_config.lr)\n",
    "\n",
    "#### scheduler\n",
    "# here we do not set eta_min to lr_min to be backward compatible\n",
    "# because in previous versions eta_min is default to 0\n",
    "# rather than the default value of lr_min 1e-6\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer,\n",
    "    optimizer_config.max_step, eta_min=optimizer_config.eta_min) # should use eta_min arg\n",
    "\n",
    "logging('=' * 100)\n",
    "for k, v in args.__dict__.items():\n",
    "    logging('    - {} : {}'.format(k, v))\n",
    "logging('=' * 100)\n",
    "logging('#params = {}'.format(args.n_all_param))\n",
    "logging('#non emb params = {}'.format(args.n_nonemb_param))\n",
    "logging('#encoder params = {}'.format(args.n_encoder_param))\n",
    "logging('#decoder params = {}'.format(args.n_decoder_param))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb76db7a",
   "metadata": {},
   "source": [
    "## Define training and evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c579a7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    # Turn on training mode which enables dropout.\n",
    "    global train_step, train_loss, best_val_loss, eval_start_time, log_start_time\n",
    "    model.train()\n",
    "\n",
    "\n",
    "    if args.model_name == 'xl' or args.model_name == 'fnetar':\n",
    "        memories = XlMemories(\n",
    "            n_stream=data_config.batch_chunk,\n",
    "            n_layer=data_config.n_layer,\n",
    "            tgt_len=data_config.tgt_len,\n",
    "            mem_len=data_config.mem_len,\n",
    "            ext_len=0,\n",
    "            dtype=next(model.parameters()).dtype\n",
    "        )\n",
    "    else:\n",
    "        memories = FeedbackMemories(n_stream=data_config.batch_chunk)\n",
    "\n",
    "    train_iter = tr_iter\n",
    "    for batch, (data, target, seq_len) in tqdm(enumerate(train_iter), total=len(train_iter) // data_config.batch_chunk):\n",
    "        model.zero_grad()\n",
    "\n",
    "        data_chunks = torch.chunk(data, data_config.batch_chunk, 0)\n",
    "        target_chunks = torch.chunk(target, data_config.batch_chunk, 0)\n",
    "        for i in range(data_config.batch_chunk):\n",
    "            data_i = data_chunks[i]\n",
    "            target_i = target_chunks[i]\n",
    "            memory_i = memories[i]\n",
    "            loss, new_memory_i = para_model(data_i, target_i, memory_i)\n",
    "            memories.update_memory_stream(stream_index=i, memory=new_memory_i)\n",
    "\n",
    "            loss = loss.float().mean().type_as(loss) / data_config.batch_chunk\n",
    "            loss.backward()\n",
    "            train_loss += loss.float().item()\n",
    "\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), optimizer_config.clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        # step-wise learning rate annealing\n",
    "        train_step += 1\n",
    "\n",
    "        # linear warmup stage\n",
    "        if train_step < optimizer_config.warmup_step:\n",
    "            curr_lr = optimizer_config.lr * train_step / optimizer_config.warmup_step\n",
    "            optimizer.param_groups[0]['lr'] = curr_lr\n",
    "\n",
    "        else:\n",
    "            scheduler.step()\n",
    "\n",
    "\n",
    "        if train_step % run_config.log_interval == 0:\n",
    "            cur_loss = train_loss / run_config.log_interval\n",
    "            elapsed = time.time() - log_start_time\n",
    "            log_str = '| epoch {:3d} step {:>8d} | {:>6d} batches | lr {:.3g} ' \\\n",
    "                      '| ms/batch {:5.2f} | loss {:5.2f}'.format(\n",
    "                epoch, train_step, batch+1, optimizer.param_groups[0]['lr'],\n",
    "                elapsed * 1000 / run_config.log_interval, cur_loss)\n",
    "            log_str += ' | ppl {:9.3f}'.format(math.exp(cur_loss))\n",
    "            logging(log_str)\n",
    "            train_loss = 0\n",
    "            log_start_time = time.time()\n",
    "\n",
    "        if train_step % run_config.eval_interval == 0:\n",
    "            val_loss = evaluate(va_iter)\n",
    "            logging('-' * 100)\n",
    "            log_str = '| Eval {:3d} at step {:>8d} | time: {:5.2f}s ' \\\n",
    "                      '| valid loss {:5.2f}'.format(\n",
    "                train_step // run_config.eval_interval, train_step,\n",
    "                (time.time() - eval_start_time), val_loss)\n",
    "            log_str += ' | valid ppl {:9.3f}'.format(math.exp(val_loss))\n",
    "            logging(log_str)\n",
    "            logging('-' * 100)\n",
    "            # Save the model if the validation loss is the best we've seen so far.\n",
    "            if not best_val_loss or val_loss < best_val_loss:\n",
    "                if not run_config.debug:\n",
    "                    with open(os.path.join(run_config.work_dir, 'model.pt'), 'wb') as f:\n",
    "                        torch.save(model, f)\n",
    "                    with open(os.path.join(run_config.work_dir, 'optimizer.pt'), 'wb') as f:\n",
    "                        torch.save(optimizer.state_dict(), f)\n",
    "                best_val_loss = val_loss\n",
    "\n",
    "            eval_start_time = time.time()\n",
    "\n",
    "        if train_step == optimizer_config.max_step:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2965150",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(eval_iter):\n",
    "    # Turn on evaluation mode which disables dropout.\n",
    "    model.eval()\n",
    "\n",
    "    # Evaluation\n",
    "    total_len, total_loss = 0, 0.\n",
    "\n",
    "    if args.model_name == 'xl' or args.model_name == 'fnetar':\n",
    "        eval_memories = XlMemories(\n",
    "            n_stream=1,\n",
    "            n_layer=data_config.n_layer,\n",
    "            tgt_len=data_config.eval_tgt_len,\n",
    "            mem_len=data_config.eval_mem_len,\n",
    "            ext_len=0,\n",
    "            dtype=next(model.parameters()).dtype\n",
    "        )\n",
    "    else:\n",
    "        eval_memories = FeedbackMemories(n_stream=1)\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for i, (data, target, seq_len) in enumerate(eval_iter):\n",
    "            if run_config.max_eval_steps > 0 and i >= run_config.max_eval_steps:\n",
    "                break\n",
    "            loss, new_eval_memory = model(data, target, eval_memories[0])\n",
    "            eval_memories.update_memory_stream(stream_index=0, memory=new_eval_memory)\n",
    "\n",
    "            loss = loss.mean()\n",
    "            total_loss += seq_len * loss.float().item()\n",
    "            total_len += seq_len\n",
    "\n",
    "    # Switch back to the training mode\n",
    "    model.train()\n",
    "\n",
    "    return total_loss / total_len"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35cfc1e",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13cf1fcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                          | 200/172045 [04:41<67:36:02,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 step      200 |    200 batches | lr 0.00025 | ms/batch 1408.33 | loss  6.91 | ppl   997.816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▏                                                                         | 400/172045 [09:24<67:28:49,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 step      400 |    400 batches | lr 0.00025 | ms/batch 1412.19 | loss  5.99 | ppl   398.943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▎                                                                         | 600/172045 [14:06<67:11:56,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 step      600 |    600 batches | lr 0.00025 | ms/batch 1413.19 | loss  5.66 | ppl   285.731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▎                                                                         | 800/172045 [18:49<67:43:29,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 step      800 |    800 batches | lr 0.00025 | ms/batch 1414.41 | loss  5.40 | ppl   220.688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▍                                                                         | 999/172045 [23:30<67:34:59,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 step     1000 |   1000 batches | lr 0.00025 | ms/batch 1413.61 | loss  5.19 | ppl   180.019\n",
      "----------------------------------------------------------------------------------------------------\n",
      "| Eval   1 at step     1000 | time: 1415.94s | valid loss  4.99 | valid ppl   147.016\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▌                                                                        | 1200/172045 [28:22<67:00:03,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 step     1200 |   1200 batches | lr 0.00025 | ms/batch 1449.95 | loss  5.02 | ppl   152.122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▌                                                                        | 1380/172045 [32:39<67:18:19,  1.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "Exiting from training early\n",
      "====================================================================================================\n",
      "| End of training | test loss  5.01 | test ppl   149.914\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Loop over epochs.\n",
    "train_step = 0\n",
    "train_loss = 0\n",
    "best_val_loss = None\n",
    "\n",
    "log_start_time = time.time()\n",
    "eval_start_time = time.time()\n",
    "\n",
    "# At any point you can hit Ctrl + C to break out of training early.\n",
    "try:\n",
    "    for epoch in itertools.count(start=1):\n",
    "        train()\n",
    "        if train_step == optimizer_config.max_step:\n",
    "            logging('-' * 100)\n",
    "            logging('End of training')\n",
    "            break\n",
    "except KeyboardInterrupt:\n",
    "    logging('-' * 100)\n",
    "    logging('Exiting from training early')\n",
    "\n",
    "# Load the best saved model.\n",
    "with open(os.path.join(run_config.work_dir, 'model.pt'), 'rb') as f:\n",
    "    model = torch.load(f)\n",
    "para_model = model.to(device)\n",
    "\n",
    "# Run on test data.\n",
    "test_loss = evaluate(te_iter)\n",
    "logging('=' * 100)\n",
    "\n",
    "logging('| End of training | test loss {:5.2f} | test ppl {:9.3f}'.format(\n",
    "    test_loss, math.exp(test_loss)))\n",
    "logging('=' * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6501ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
